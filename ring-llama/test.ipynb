{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modeling_llama import LlamaForCausalLM\n",
    "from tokenization_llama_fast import LlamaTokenizerFast\n",
    "\n",
    "def load_model(model_name: str, cache_dir: str):\n",
    "    tokenizer = LlamaTokenizerFast.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name, cache_dir=cache_dir, torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my LlamaModel\n",
      "_attn_implementation flash_attention_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.25s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaFlashAttention2(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model, tokenizer = load_model(\"meta-llama/Llama-2-7b-chat-hf\", cache_dir='/workspace/hf_home/')\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x {'input_ids': tensor([[    1, 15043,   306,   626,   278, 11148,  3304, 29892, 29871]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "y tensor([[    1, 15043,   306,   626,   278, 11148,  3304, 29892, 29871, 29906,\n",
      "         29900, 29896, 29929,    13, 29930, 15918,  1596,   373,  5650, 19239,\n",
      "           373,  8112,  9451,   411,  1274,   719,   506, 10675,   322, 18187,\n",
      "         29889,    13, 10994,   306,  1913,   450,   365, 29880,  3304,   313,\n",
      "         16432,   511, 29871, 29906, 29900, 29896, 29929,    13, 27103,  1596,\n",
      "           373,  5650, 19239,   373,  8112,  9451,   411,  1274,   719,   506,\n",
      "         10675,   322, 18187, 29889,     2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = tokenizer(\"Hello I am the llama, \", return_tensors=\"pt\")\n",
    "print(\"x\", x)\n",
    "x = x.to(device)\n",
    "y = model.generate(x.input_ids, min_new_tokens=1,  max_new_tokens=128, do_sample=True,  temperature=1.0, repetition_penalty=1.2)\n",
    "print(\"y\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> Hello I am the llama, 2019\\n* Digital print on paper mounted on wood panel with acrylic paint and fabric.\\nHello I Am The Llama (detail), 2019\\nDigital print on paper mounted on wood panel with acrylic paint and fabric.</s>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(sequences=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
