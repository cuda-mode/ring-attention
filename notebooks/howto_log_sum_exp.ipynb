{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Howto Log Sum Exp\n",
    "\n",
    "Using flash-attention intensively you will at some point hear about `lse` values being returend. `lse` or log sum exp can be used to compute softmax (and thereby also attention) in a blockwise fashion. This notebook aaims to explain how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining a naive softmax function .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Sequence\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def naive_softmax(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x.exp() / x.exp().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and verify that it's output matches the output of the official `torch.softmax()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor([0.0113, 0.0985, 0.1366, 0.1658, 0.2436, 0.0925, 0.0238, 0.0939, 0.0608,\n",
      "        0.0733])\n",
      "b tensor([0.0113, 0.0985, 0.1366, 0.1658, 0.2436, 0.0925, 0.0238, 0.0939, 0.0608,\n",
      "        0.0733])\n",
      "allcclose True\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10)  # generate normally distributed random numbers\n",
    "a = torch.softmax(x, dim=-1) # reference output\n",
    "b = naive_softmax(x) # our naive version\n",
    "\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "print(\"allcclose\", torch.allclose(a, b, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our naive softmax function has a problem with numerical stability when it get input vectors with larger elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., nan, nan, 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_softmax(x * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we will look into how to fix this let's first look at how a blockwise computation of softmax can be implemented with `naive_softmax()`. We generate a vector and split it into two chunks of equal size and compute softmax of the chunks individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have:\n",
      "s1 = tensor([0.1411, 0.1636, 0.0206, 0.6230, 0.0518])\n",
      "s2 = tensor([0.1150, 0.4318, 0.0457, 0.0820, 0.3254])\n",
      "We want:\n",
      "target = tensor([0.0586, 0.0118, 0.0048, 0.0119, 0.0144, 0.0147, 0.0370, 0.0435, 0.0064,\n",
      "        0.0116, 0.0723, 0.0200, 0.0224, 0.1518, 0.3166, 0.0333, 0.0943, 0.0225,\n",
      "        0.0115, 0.0405])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10)\n",
    "\n",
    "x1,x2 = torch.chunk(x, 2)\n",
    "s1 = naive_softmax(x1)\n",
    "s2 = naive_softmax(x2)\n",
    "\n",
    "print(\"We have:\")\n",
    "print(f\"s1 = {s1}\")\n",
    "print(f\"s2 = {s2}\")\n",
    "\n",
    "target = naive_softmax(x)\n",
    "print(\"We want:\")\n",
    "print(f\"target = {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at `naive_softmax()` we note that its output is divided by `x.exp().sum()`. We can call this the \"sum exp\" value (note the similarity to \"log sum exp\") and we can use it to \"undo\" the softmax normalization and thereby compute combine multiple softmax chunks if we have this vaue for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After correction with help of lse values:\n",
      "s_combined tensor([0.0505, 0.0586, 0.0074, 0.2232, 0.0186, 0.0738, 0.2771, 0.0293, 0.0526,\n",
      "        0.2088])\n",
      "allclose(s_combined, target): True\n"
     ]
    }
   ],
   "source": [
    "se_x1 = x1.exp().sum()\n",
    "se_x2 = x2.exp().sum()\n",
    "s1_corrected = s1 * se_x1 / (se_x1 + se_x2)\n",
    "s2_corrected = s2 * se_x2 / (se_x1 + se_x2)\n",
    "\n",
    "print(\"After correction with help of lse values:\")\n",
    "s_combined = torch.cat([s1_corrected, s2_corrected])\n",
    "print(\"s_combined\", s_combined)\n",
    "\n",
    "print(\"allclose(s_combined, target):\", torch.allclose(s_combined, target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but is this helpful at all? Yes, and it becomes more obivous when we realize that we can return this value from our softmax function and we can do the correction in a blockwise fashion in a loop by accumulating the lse value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0500, 0.0066, 0.0234, 0.0261, 0.0528, 0.0184, 0.0217, 0.1169, 0.0282,\n",
      "        0.0131, 0.0274, 0.0812, 0.0592, 0.4354, 0.0068, 0.0071, 0.0023, 0.0067,\n",
      "        0.0113, 0.0054])\n",
      "tensor([0.0500, 0.0066, 0.0234, 0.0261, 0.0528, 0.0184, 0.0217, 0.1169, 0.0282,\n",
      "        0.0131, 0.0274, 0.0812, 0.0592, 0.4354, 0.0068, 0.0071, 0.0023, 0.0067,\n",
      "        0.0113, 0.0054])\n"
     ]
    }
   ],
   "source": [
    "def naive_softmax2(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    exp_sum = x.exp().sum()\n",
    "    return x.exp() / exp_sum, exp_sum\n",
    "\n",
    "\n",
    "def naive_blockwise_softmax(blocks: Sequence[torch.Tensor]) -> torch.Tensor:\n",
    "    out = []\n",
    "    exp_sum = 0\n",
    "    for block in blocks:\n",
    "        block_softmax, block_sum_exp = naive_softmax2(block)\n",
    "        for o in out:\n",
    "            o *= exp_sum / (exp_sum + block_sum_exp)\n",
    "        \n",
    "        out.append(block_softmax * block_sum_exp / (block_sum_exp + exp_sum))\n",
    "        exp_sum += block_sum_exp\n",
    "        \n",
    "    return torch.cat(out)\n",
    "    \n",
    "\n",
    "x_long = torch.randn(20)\n",
    "chunks = torch.chunk(x_long, 4)\n",
    "print(naive_blockwise_softmax(chunks))\n",
    "print(torch.softmax(x_long, dim=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, then now let's look at the numerical stability of softmax. First we can observe a interesting property of the softmax function: it's output is shift/translation invariant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2243, 0.1702, 0.1054, 0.1254, 0.0674, 0.1519, 0.0710, 0.0844])\n",
      "tensor([0.2243, 0.1702, 0.1054, 0.1254, 0.0674, 0.1519, 0.0710, 0.0844])\n",
      "tensor([0.2243, 0.1702, 0.1054, 0.1254, 0.0674, 0.1519, 0.0710, 0.0844])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(8)\n",
    "print(naive_softmax(x))\n",
    "print(naive_softmax(x+5))\n",
    "print(naive_softmax(x-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This porperty allows us to deal with problematic large inputs simply by subtracting their maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_softmax(x):\n",
    "    m = x.max()\n",
    "    return (x-m).exp() / (x-m).exp().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"stable\" function now can also deal with larger value that were problematic for our naive function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive:  tensor([nan, nan, 0., 0., 0., 0., nan, 0., 0., 0.])\n",
      "stable:  tensor([8.8560e-23, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        8.3795e-27, 0.0000e+00, 0.0000e+00, 0.0000e+00])\n",
      "torch:  tensor([8.8560e-23, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        8.3795e-27, 0.0000e+00, 0.0000e+00, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "large_input = torch.randn(10) * 100\n",
    "\n",
    "print(\"naive: \", naive_softmax(large_input))\n",
    "print(\"stable: \", stable_softmax(large_input))\n",
    "print(\"torch: \", torch.softmax(large_input, dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_softmax2(x):\n",
    "    \"\"\"returns softmax result and log sum exp\"\"\"\n",
    "    m = x.max()\n",
    "    a = (x - m).exp()\n",
    "    b = a.sum()\n",
    "    lse = m + torch.log(b)\n",
    "    return a / b, lse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can now use this to combine two softmax block results, but to do it in the same way as before we would need to calculate the exp() values.. which is as we know numerically not stable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0629, 0.0096, 0.0210, 0.0297, 0.0161, 0.0350, 0.0121, 0.0678, 0.0051,\n",
      "        0.0677, 0.0215, 0.0045, 0.0189, 0.1579, 0.0195, 0.1159, 0.0335, 0.0363,\n",
      "        0.2333, 0.0315])\n",
      "tensor([0.0629, 0.0096, 0.0210, 0.0297, 0.0161, 0.0350, 0.0121, 0.0678, 0.0051,\n",
      "        0.0677, 0.0215, 0.0045, 0.0189, 0.1579, 0.0195, 0.1159, 0.0335, 0.0363,\n",
      "        0.2333, 0.0315]) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(20)\n",
    "\n",
    "a = torch.softmax(x, dim=-1)\n",
    "\n",
    "x1, x2 = x.chunk(2)\n",
    "\n",
    "b1, lse1 = stable_softmax2(x1)\n",
    "b2, lse2 = stable_softmax2(x2)\n",
    "\n",
    "c1 = b1 * torch.exp(lse1) / (torch.exp(lse1) + torch.exp(lse2))\n",
    "c2 = b2 * torch.exp(lse2) / (torch.exp(lse1) + torch.exp(lse2))\n",
    "\n",
    "print(a)\n",
    "print(torch.cat([c1, c2]), torch.allclose(a, torch.cat([c1, c2])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But luckily log & exp are to the rescue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0629, 0.0096, 0.0210, 0.0297, 0.0161, 0.0350, 0.0121, 0.0678, 0.0051,\n",
      "        0.0677, 0.0215, 0.0045, 0.0189, 0.1579, 0.0195, 0.1159, 0.0335, 0.0363,\n",
      "        0.2333, 0.0315])\n",
      "tensor([0.0629, 0.0096, 0.0210, 0.0297, 0.0161, 0.0350, 0.0121, 0.0678, 0.0051,\n",
      "        0.0677, 0.0215, 0.0045, 0.0189, 0.1579, 0.0195, 0.1159, 0.0335, 0.0363,\n",
      "        0.2333, 0.0315]) True\n"
     ]
    }
   ],
   "source": [
    "d1 = b1 * torch.exp(-torch.log(1 + torch.exp(lse2 - lse1)))\n",
    "d2 = b2 * torch.exp(-torch.log(1 + torch.exp(lse1 - lse2)))\n",
    "print(a)\n",
    "print(torch.cat([d1, d2]), torch.allclose(a, torch.cat([d1, d2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why `b1 * torch.exp(lse1) / (torch.exp(lse1) + torch.exp(lse2))` is equal to `b1 * torch.exp(-torch.log(1 + torch.exp(lse2-lse1)))` we remember school math basics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math.exp(5)/math.exp(3) = 7.38905609893065\n",
      "math.exp(5 - 3) = 7.38905609893065\n",
      "a/(a+b) = 0.625\n",
      "1/(1+b/a) = 0.625\n"
     ]
    }
   ],
   "source": [
    "a = 5\n",
    "b = 3\n",
    "\n",
    "print(\"math.exp(5)/math.exp(3) =\", math.exp(5) / math.exp(3))\n",
    "print(\"math.exp(5 - 3) =\", math.exp(5 - 3))\n",
    "\n",
    "print(\"a/(a+b) =\", a / (a+b))\n",
    "print(\"1/(1+b/a) =\", 1 / (1+b/a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the fresh knowledge about softmax we can now take a look at the `update()` function that is used in the ring-flash-attention implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_out_and_lse(\n",
    "    out: torch.Tensor,\n",
    "    lse: torch.Tensor,\n",
    "    block_out: torch.Tensor,\n",
    "    block_lse: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    block_out = block_out.to(torch.float32)\n",
    "    block_lse = block_lse.transpose(-2, -1).unsqueeze(dim=-1)\n",
    "\n",
    "    new_lse = lse + torch.log(1 + torch.exp(block_lse - lse))\n",
    "    out = torch.exp(lse - new_lse) * out + torch.exp(block_lse - new_lse) * block_out\n",
    "\n",
    "    lse = new_lse\n",
    "    return out, lse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
